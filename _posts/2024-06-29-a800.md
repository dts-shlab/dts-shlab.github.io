---
layout: post
title: 由A800平台训练InternLM-7B无法收敛引发的思考
date: 2024-06-29 12:00:00
tags: Training System
categories: LLMs
redirect: https://zhuanlan.zhihu.com/p/701623664
---

## Abstract

大模型训练（如InternLM-7B）实践中，曾经遇到过在A100集群上表现正常的代码和数据，迁移到A800集群却出现了模型准确度下降和梯度范数爆炸的问题。经过调查，我们发现这与A800和A100 GPU的NVLink带宽差异有关。通过在两个集群上使用nanoGPT模型进行的对照实验，我们确认了精度差异的原因在于NCCL的Ring all-reduce算法实现。进一步实验表明，设置环境变量NCCL_ALGO=Tree或使用gloo作为backend可以解决精度对齐问题。最终，我们提出了一个解决方案：在A800集群上设置NCCL_ALGO=Tree，强制使用Tree算法进行all-reduce操作，从而避免了Ring算法带来的精度问题，使得A800集群的模型能够正常收敛，并且与A100集群的训练精度对齐。

