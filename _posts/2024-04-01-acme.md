---
layout: post
title: Understanding the Workload Characteristics of Large Language Model Development
date: 2024-04-19 12:00:00
tags: Training System
categories: LLMs
redirect: https://www.usenix.org/publications/loginonline/understanding-workload-characteristics-large-language-model-development
---

## Abstract

Large Language Models (LLMs) have presented impressive performance across several transformative tasks, such as chatbot and code generation. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. A thorough analysis of cluster workloads is essential for comprehending challenges and uncovering opportunities in designing systems tailored for LLMs.

To this end, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme of Shanghai AI Laboratory. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures.