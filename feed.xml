<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://nds-shlab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nds-shlab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-29T11:59:38+00:00</updated><id>https://nds-shlab.github.io/feed.xml</id><title type="html">blank</title><subtitle>NDS Group @ Shanghai AI Lab </subtitle><entry><title type="html">Understanding the Workload Characteristics of Large Language Model Development</title><link href="https://nds-shlab.github.io/blog/2024/acme/" rel="alternate" type="text/html" title="Understanding the Workload Characteristics of Large Language Model Development"/><published>2024-04-19T12:00:00+00:00</published><updated>2024-04-19T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/acme</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/acme/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Large Language Models (LLMs) have presented impressive performance across several transformative tasks, such as chatbot and code generation. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. A thorough analysis of cluster workloads is essential for comprehending challenges and uncovering opportunities in designing systems tailored for LLMs.</p> <p>To this end, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme of Shanghai AI Laboratory. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures.</p>]]></content><author><name></name></author><category term="LLMs"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>